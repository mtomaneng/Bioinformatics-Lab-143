---
title: "Class09"
author: "Matthew Tomaneng"
date: "5/1/2018"
output:
  html_document:
    keep_md: yes 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
wisc.df <- read.csv("~/Downloads/WisconsinCancer.csv")
```


#converting features of wisc.df
```{r}
wisc.data <- as.matrix(wisc.df[,3:32])
row.names(wisc.data) <- wisc.df$id
head(wisc.data)
```
# How many M's do we have
```{r}
table(wisc.df$diagnosis)
```
#Making a new vector called diagnosis
```{r}
diagnosis <- as.numeric(wisc.df$diagnosis == "M")
table(diagnosis)
```
#Should match previous table



## Exploratory data analysis
Q1. How many observations are in this dataset?
```{r}
nrow(wisc.df)
```

Q2. How many variables/features in the data are suffixed with _mean?
```{r}
grep("mean", colnames(wisc.data))
#produces a drop down of the differnt variables with _mean in the name
#12 is how many "_mean" columns we have
```

Q3. How many of the observations have a malignant diagnosis?
```{r}
sum(diagnosis)
#212 Malignant diagnosis 
```

##Section 2
#Check if rescaling is needed
```{r}
# Check column means and standard deviations
colMeans(wisc.data)

apply(wisc.data,2,sd)
```

#Using PCA on wisc.data
```{r}
wisc.pr <- prcomp(wisc.data, scale = TRUE)
summary(wisc.pr)
```

#Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?
There is a .4427 original variance

#Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?
All the way to PC3 will have = 72.6%

#How many principal components (PCs) are required to describe at least 90% of the original variance in the data?
You would need all the way up to PC7 to get 91% of the original variance

##Interpreting the Results
```{r}
#Try to plot our info
biplot(wisc.pr, scale = TRUE)
```
#Q7 What stands out to you about this plot? Is it easy or difficult to understand? Why?
This plot is too convoluted with data to actually read any individual point.
```{r}
# Scatter plot observations by components 1 and 2
attributes
#We want "X"
plot(wisc.pr$x[, c(1,2)], col = (diagnosis +1),
     xlab = "PC1", ylab = "PC2")
```

#Q8. Repeat the same for principal components 1 and 3. What do you notice about these plots?

```{r}
# Repeat for components 1 and 3
plot(wisc.pr$x[, c(1, 3)], col = (diagnosis + 1), 
     xlab = "PC1", ylab = "PC3")

```
All the data is now on a higher range vertially. There is a greater spread

```{r}
pr.var <- wisc.pr$sdev^2
pve <- pr.var/sum(pr.var)
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained", 
#Names.arg labels x axis values (PC#)
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

##Variance Scree Plot
```{r}
#Calculate varianceof each component
pr.var <- wisc.pr$sdev^2
pve <- pr.var/sum(pr.var)

# Plot cumulative proportion of variance explained
plot( pve , xlab = "Principal Component", 
     ylab = "Cumulative Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")

```

##Clustering
```{r}
wisc.clust <-hclust(dist(scale(wisc.data)))

wisc.hclust.clusters <- cutree(wisc.clust, k=4)

#Plot the histogram
plot(wisc.clust)
abline(h = 20, col = "red")


```

How may M or 1 (cancer) and 0 (non-cancer) arein each cluster?

```{r}
table(wisc.hclust.clusters, diagnosis)
```

#Q12. Can you find a better cluster vs diagnoses match with by cutting into a different number of clusters between 2 and 10?

##K means
```{r}
wisc.km <- kmeans(scale(wisc.data), centers= 2, nstart = 20)
table(wisc.km$cluster, diagnosis)
```



Is this better than histogram?

```{r}
table(wisc.hclust.clusters , wisc.km$cluster)
```

##Try again except with more PC values

```{r}
## Use the distance along the first 7 PCs for clustering i.e. wisc.pr$x[, 1:7]
wisc.pr.hclust <- hclust(dist(wisc.pr$x[,1:7]), method= "complete")
plot(wisc.pr.hclust)
```
Cut the tree down
```{r}
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k= 4)

table(wisc.pr.hclust.clusters, diagnosis)
```
Compare all 3 methods
```{r}
table(wisc.km$cluster, diagnosis)
table(wisc.hclust.clusters, diagnosis)
```

##Specificity vs. Selectivity
Sensitivity
- km seems to have the best sensitivity with a score of .7783 on sucessfully identifying sick patients
Selectivity
- The PC7 trial had the best specificity by rejecting most of the benign cases